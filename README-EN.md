# Pronunciation Mapper

A Korean-English pronunciation similarity-based mapping system designed for **Query Rewriting to accurately match ASR (Automatic Speech Recognition) output with database terms**. Specifically optimized for **Lexical Search (keyword-based search) environments where exact keyword matching of proper nouns, technical terms, and database field names is essential**, with additional benefits for improving Semantic Search (vector search) performance.

![Python Version](https://img.shields.io/badge/python-3.6-blue.svg) ~ ![Python Version](https://img.shields.io/badge/python-3.11-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## Introduction

When querying databases through voice interfaces, mismatches between ASR output and structured data terminology can cause significant issues. This library uses **Query Rewriting techniques** to resolve inconsistencies such as:

- **STT Error Correction**: "íŠ¸ëœì­ìˆ‘" â†’ "transaction"
- **Korean-English Pronunciation Conversion**: "ì»¤ìŠ¤í„°ë¨¸" â†’ "customer"
- **Proper Noun Normalization**: "ì„œë¹„ìŠ¤ì–´ì¹´ìš´íŠ¸" â†’ "svc_accnt"
- **Abbreviation Expansion**: "ë””ë¹„" â†’ "ë°ì´í„°ë² ì´ìŠ¤"

This enables **accurate keyword matching of proper nouns and technical terms in Lexical Search (BM25, TF-IDF, Elasticsearch, etc.)**, and additionally improves Semantic Search accuracy by normalizing queries during the embedding preprocessing stage.

### Query Rewriting and Search Performance Enhancement

#### Lexical Search (Keyword Search) - Primary Goal ğŸ¯
```
Before: "ì»¤ìŠ¤í„°ë¨¸ ë°ì´í„°" â†’ No "customer" in DB â†’ Search fails
After:  "customer ë°ì´í„°" â†’ "customer" exists in DB â†’ Search succeeds
```
- **Specialized for proper nouns and technical terms requiring exact keyword matching**
- Guarantees accurate matching of database field names, service names, account IDs, etc.
- Perfect compatibility with keyword-based search engines like BM25, Elasticsearch, TF-IDF
- Examples: Searching for proper nouns like "account_no", "XPN36", "ST Corporation"

#### Semantic Search (Vector Search) - Additional Benefit
```
Before: "íŠ¸ëœì­ìˆ‘ ë¡œê·¸" â†’ [embedding] â†’ Less relevant results
After:  "transaction ë¡œê·¸" â†’ [embedding] â†’ Accurate results
```
- Embedding with correct terms improves vector similarity
- Prevents embedding quality degradation from typos or pronunciation errors

#### Hybrid Search
```
Lexical Search (70%) + Semantic Search (30%) weighted average
â†’ Query Rewriting significantly improves Lexical Search accuracy
```

### Key Features

- **Proper noun and technical term normalization**: Accurate keyword matching for Lexical Search
- Korean-English pronunciation similarity-based mapping
- Korean pronunciation analysis through jamo decomposition
- Similarity calculation using Levenshtein distance
- Automatic term conversion in sentences (Query Rewriting)
- User-defined mapping support
- Command-line interface

## Installation

```bash
# Install from source
git clone https://github.com/yourusername/pronunciation-mapper.git
cd pronunciation-mapper
pip install -e .
```

## Quick Start

### Basic Usage

Use PronunciationMapper as follows:

```python
from pronunciation_mapper import PronunciationMapper

# Define DB structured data terms (Lexical Search targets)
db_terms = [
    'customer', 'product', 'transaction',
    'payment', 'shipping', 'invoice',
    'ground',  'server',
    'ë°ì´í„°ë² ì´ìŠ¤', 'í…Œì´ë¸”', 'í•„ë“œ',
    'ì¸ë±ìŠ¤', 'ì¿¼ë¦¬', 'svc66','log','account_no','account_id', 'ì„œë²„',
    'konlpy', 'XPN36', 'STì£¼ì‹íšŒì‚¬','KFì£¼ì‹íšŒì‚¬','SFì£¼ì‹íšŒì‚¬','SMTA','svc_accnt'
]

# Add custom vocabulary mappings from dictionary
custom_mappings = {
    # Korean voice initial output -> DB index dictionary proper noun mapping (Query Rewriting)
    'ì„œë¹„ìŠ¤ì–´ì¹´ìš´íŠ¸': 'svc_accnt',
    'ì—ìŠ¤í‹°ì£¼ì‹íšŒì‚¬':'STì£¼ì‹íšŒì‚¬',
    'ì–´ì»¤ìš´íŠ¸ë„˜ë²„': 'account_no',
    'ì–´ì¹´ìš´íŠ¸ì•„ì´ë””': 'account_id',
    'ì–´ì¹´ì•„ì´ë””': 'account_id',
    'íŠ¸ëœì­ì…˜': 'transaction',
    'í˜ì´ë¨¼íŠ¸': 'payment',
    'ì‰¬í•‘': 'shipping',
    'ì¸ë³´ì´ìŠ¤': 'invoice',
    'ê·¸ë¼ìš´ë“œ': 'ground',
    'í´ë¼ìš°ë“œ': 'cloud',
    'ì„œë²„': 'server',
    'ì—‘ìŠ¤í”¼ì—”36': 'XPN36',
    'ì—‘ìŠ¤í”¼ì—”ì‚¼ì‹¬ìœ¡': 'XPN36',
    'ì¼€ì´ì—í”„ì£¼ì‹íšŒì‚¬':'KFì£¼ì‹íšŒì‚¬'
}

# Initialize mapper
mapper = PronunciationMapper(db_terms, custom_mappings=custom_mappings)

# Single term mapping (Query Rewriting)
result, score = mapper.find_closest_term("ì»¤ìŠ¤í„°ë¨¸")

# Sentence mapping (Query Rewriting)
sentence = "ê·¸ë¼ìš´ë“œì— ìˆëŠ” ì—‘ìŠ¤í”¼ì—”36 ë°ì´íƒ€ë°°ì´ìŠ¤ ì¨ë²„ì˜ íŠ¸ëœí…ìˆ‘ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì„œë¹„ìŠ¤ìœ¡ì‹­ìœ¡ ìƒí’ˆì—ì„œ ì‚¼ë°±ì´ì‹­ì¼ë²ˆ íŠ¸ëœì ì…˜ ë¡œê·¸ ì°¾ì•„ì¤˜ ì–´ì¹´ìš´íŠ¸ë„˜ë²„ ì‚¬ì‚¼ì‚¼ì˜¤ì‚¼ì¹  ì²œêµ­ì˜ê³„ë‹¨. ë‚˜ëŠ” ì—ìŠ¤í‹°ì£¼ì‹íšŒì‚¬ ì²œë§Œë°±ë¶€ì¥ì´ê³  ì–´ì¹´ì•„ì´ë””ëŠ” ì•„ë‹ˆì•„ë‹ˆ ì–´ì¹´ìš´íŠ¸ì•„ì´ë””ëŠ” ê³µíŒ”ê³µíŒ”íŒ”ì´ì•¼"
mapped = mapper.map_sentence(sentence)
```

**Single Term Mapping Result:**
```
Mapping result: customer, proximity score: 0.0, similarity: 1.00
```

**Sentence Mapping Result:**
```
Initial STT output: ê·¸ë¼ìš´ë“œì— ìˆëŠ” ì—‘ìŠ¤í”¼ì—”36 ë°ì´íƒ€ë°°ì´ìŠ¤ ì¨ë²„ì˜ íŠ¸ëœí…ìˆ‘ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì„œë¹„ìŠ¤ìœ¡ì‹­ìœ¡ ìƒí’ˆì—ì„œ ì‚¼ë°±ì´ì‹­ì¼ë²ˆ íŠ¸ëœì ì…˜ ë¡œê·¸ ì°¾ì•„ì¤˜ ì–´ì¹´ìš´íŠ¸ë„˜ë²„ ì‚¬ì‚¼ì‚¼ì˜¤ì‚¼ì¹  ì²œêµ­ì˜ê³„ë‹¨. ë‚˜ëŠ” ì—ìŠ¤í‹°ì£¼ì‹íšŒì‚¬ ì²œë§Œë°±ë¶€ì¥ì´ê³  ì–´ì¹´ì•„ì´ë””ëŠ” ì•„ë‹ˆì•„ë‹ˆ ì–´ì¹´ìš´íŠ¸ì•„ì´ë””ëŠ” ê³µíŒ”ê³µíŒ”íŒ”ì´ì•¼

After Query Rewriting: groundì— ìˆëŠ” XPN36 ë°ì´í„°ë² ì´ìŠ¤ serverì˜ transaction ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì„œë¹„ìŠ¤ìœ¡ì‹­ìœ¡ ìƒí’ˆì—ì„œ 321ë²ˆ transaction log ì°¾ì•„ì¤˜ account_no 433537 ì²œêµ­ì˜ê³„ë‹¨. ë‚˜ëŠ” STì£¼ì‹íšŒì‚¬ ì²œë§Œë°±ë¶€ì¥ì´ê³  account_idëŠ” ì•„ë‹ˆì•„ë‹ˆ account_idëŠ” 080882ì•¼
```

### Command-Line Tool Usage

```bash
# Single word mapping
pronunciation-mapper map-word ì»¤ìŠ¤í„°ë¨¸
# Output: ì»¤ìŠ¤í„°ë¨¸ â†’ customer (similarity: 1.00)

# Sentence mapping (Query Rewriting)
pronunciation-mapper map-sentence "ê·¸ë¼ìš´ë“œì— ìˆëŠ” ë°ì´íƒ€ë² ì´ìŠ¤ í™•ì¸"
# Output:
# Original: ê·¸ë¼ìš´ë“œì— ìˆëŠ” ë°ì´íƒ€ë² ì´ìŠ¤ í™•ì¸
# After Query Rewriting: groundì— ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸

# Add user mapping
pronunciation-mapper add-mapping ê³ ê° customer --save
# Output: Mapping added: ê³ ê° â†’ customer
#         Mapping saved: /home/user/.pronunciation_mapper/mapping_cache.json
```

## How It Works

1. **Pronunciation Normalization**: Convert input words and DB terms to pronunciation form
   - Korean: Jamo decomposition (e.g., 'ì•ˆë…•' â†’ 'ã…‡ã…ã„´ã„´ã…•ã…‡')
   - English: Korean pronunciation mapping (e.g., 'cloud' â†’ 'ã…‹ã„¹ã…ã…œã„·')

2. **Similarity Calculation**: Measure string similarity based on Levenshtein distance
   - Normalized distance = edit distance / max(len(s1), len(s2))

3. **Optimal Mapping Selection**: Choose the closest term with similarity above threshold

## RAG System Integration Example

### Proper Noun-Centric Lexical Search Pipeline

```python
from pronunciation_mapper import PronunciationMapper
from elasticsearch import Elasticsearch

# 1. Initialize Pronunciation Mapper (Query Rewriting)
db_terms = ["account_no", "account_id", "transaction", "XPN36", "svc_accnt"]
mapper = PronunciationMapper(db_terms, custom_mappings={
    "ì–´ì¹´ìš´íŠ¸ë„˜ë²„": "account_no",
    "ì–´ì¹´ìš´íŠ¸ì•„ì´ë””": "account_id",
    "íŠ¸ëœì­ì…˜": "transaction",
    "ì—‘ìŠ¤í”¼ì—”36": "XPN36",
    "ì„œë¹„ìŠ¤ì–´ì¹´ìš´íŠ¸": "svc_accnt"
})

# 2. User query (STT output) - with proper nouns
user_query = "ì—‘ìŠ¤í”¼ì—”36 ì„œë²„ì—ì„œ ì–´ì¹´ìš´íŠ¸ë„˜ë²„ ì‚¬ì‚¼ì‚¼ì˜¤ì‚¼ì¹ ì˜ íŠ¸ëœì­ìˆ‘ ë¡œê·¸"

# 3. Apply Query Rewriting - normalize proper nouns
rewritten_query = mapper.map_sentence(user_query)
# Result: "XPN36 ì„œë²„ì—ì„œ account_no 433537ì˜ transaction ë¡œê·¸"

# 4. Lexical Search (keyword search) - exact proper noun matching
es = Elasticsearch()
lexical_results = es.search(index="logs", body={
    "query": {
        "bool": {
            "must": [
                {"match": {"server_name": "XPN36"}},      # Exact server name
                {"match": {"account_no": "433537"}},      # Exact account number
                {"match": {"log_type": "transaction"}}    # Exact log type
            ]
        }
    }
})

# 5. Optional: Add Semantic Search (hybrid)
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.load_local("./vectorstore", embeddings)
semantic_results = vectorstore.similarity_search(rewritten_query, k=3)

# 6. Hybrid Search (Lexical priority)
# Lexical Search finds proper nouns accurately, Semantic Search supplements context
final_results = combine_results(
    lexical_results,    # 70% weight - proper noun matching
    semantic_results,   # 30% weight - semantic similarity
    weights=[0.7, 0.3]
)
```

### Lexical Search Performance Comparison (Proper Noun Search)

```python
# Without Query Rewriting - proper noun matching fails
query = "ì„œë¹„ìŠ¤ì–´ì¹´ìš´íŠ¸ ì‚¬ì‚¼ì‚¼ì˜¤ì‚¼ì¹ "
# Elasticsearch BM25 search â†’ 0 results (keyword matching fails)
# Reason: Cannot find "svc_accnt" and "433537"

# With Query Rewriting - accurate proper noun matching
rewritten = mapper.map_sentence(query)  # "svc_accnt 433537"
# Elasticsearch BM25 search â†’ 25 results (accurate keyword matching)
# Reason: Database field names and account numbers match exactly

# Real-world example: AWS monitoring
query = "ì—‘ìŠ¤í”¼ì—”36 ì¨ë²„ íŠ¸ëœì­ìˆ‘ ë¡œê·¸"
rewritten = "XPN36 server transaction log"
# â†’ Accurately searches "XPN36" (server name), "transaction" (log type)
```

## Advanced Usage

### Adding Custom Mappings

```python
from pronunciation_mapper import PronunciationMapper

# Initial DB terms
db_terms = ["customer", "product", "database"]

# Custom mappings (Query Rewriting rules)
custom_mappings = {
    "ê³ ê°": "customer",
    "ì œí’ˆ": "product",
    "ë””ë¹„": "database"  # Handle abbreviation
}

# Initialize mapper (apply custom mappings)
mapper = PronunciationMapper(db_terms, custom_mappings=custom_mappings)

# Test mapping
result, score = mapper.find_closest_term("ë””ë¹„")
print(f"Mapping result: {result}, score: {score}")
# Output: Mapping result: database, score: 0.0
```

### Adjusting Threshold

```python
# Stricter matching (higher accuracy)
strict_match, score = mapper.find_closest_term("ë°ì´íƒ€ë² ì´ìŠ¤", threshold=0.3)

# More flexible matching (allow more variations)
flexible_match, score = mapper.find_closest_term("ë°ì´íƒ€ë² ì´ìŠ¤", threshold=0.7)
```

## ASR System Integration

The pronunciation mapper easily integrates with ASR systems to convert speech recognition results to accurate proper nouns and technical terms.

```python
# ASR result processing example (Query Rewriting) - proper noun focus
asr_result = "ì—‘ìŠ¤í”¼ì—”36 ì„œë²„ì—ì„œ ì–´ì¹´ìš´íŠ¸ë„˜ë²„ ì‚¬ì‚¼ì‚¼ì˜¤ì‚¼ì¹  ì¡°íšŒ"
db_query = mapper.map_sentence(asr_result)
# Result: "XPN36 ì„œë²„ì—ì„œ account_no 433537 ì¡°íšŒ"

# Now proper nouns match exactly for Lexical Search
# 1. Lexical Search (primary):
#    - server_name = "XPN36" (exact server name)
#    - account_no = "433537" (exact account number)
# 2. Semantic Search (supplementary): embedding(db_query)
```

## Difference Between DB_TERMS and CUSTOM_MAPPINGS

### Quick Summary

- **DB_TERMS**: "What to find" - **target term list** (core search targets for Lexical Search)
- **CUSTOM_MAPPINGS**: "What to convert to what" - **Query Rewriting rules**

### Easy Analogy: Dictionary vs Directory

- **DB_TERMS** is like a **phone directory**:
  - List of people/companies that actually exist
  - These indexed terms are proper nouns in structured data (account numbers, server names, field names, etc.)
  - **Keywords that must be exactly matched in Lexical Search**

- **CUSTOM_MAPPINGS** is like a **translation dictionary**:
  - Query Rewriting rules saying "translate this word to that word"
  - Defines how to transform

### How It Actually Works

1. **DB_TERMS** (target term list - Lexical Search keywords):
   ```python
   db_terms = ["account_no", "XPN36", "transaction"]
   ```
   - List of 'official terms' the system can recognize and map
   - Final conversion results can only be terms in this list
   - "Proper nouns searchable in our database/system"
   - **Keywords that must be exactly matched in Lexical Search**

2. **CUSTOM_MAPPINGS** (Query Rewriting rules - proper noun conversion):
   ```python
   custom_mappings = {
     "ì–´ì¹´ìš´íŠ¸ë„˜ë²„": "account_no",
     "ì—‘ìŠ¤í”¼ì—”36": "XPN36",
     "ì—‘ìŠ¤í”¼ì—”ì‚¼ì‹¬ìœ¡": "XPN36",
     "íŠ¸ëœì­ì…˜": "transaction",
     "íŠ¸ëœì ì…˜": "transaction",
     "ì„œë¹„ìŠ¤ì–´ì¹´ìš´íŠ¸": "svc_accnt"
   }
   ```
   - Defines which input word (STT output) converts to which official term (DB proper noun)
   - When word on left (key) appears, convert to right (value)
   - "Proper noun conversion rules"

### Relationship Between the Two

- **Core principle**: CUSTOM_MAPPINGS **values (right side) must be in DB_TERMS** to match in Lexical Search
- Example:
  ```python
  db_terms = ["account_no", "XPN36"]
  custom_mappings = {"ì–´ì¹´ìš´íŠ¸ë„˜ë²„": "account_no", "ì„œë²„ì´ë¦„": "server_name"}
  ```
  - "ì–´ì¹´ìš´íŠ¸ë„˜ë²„" maps to "account_no" (success, "account_no" is in db_terms)
  - "ì„œë²„ì´ë¦„" doesn't map to "server_name" (fail, "server_name" is not in db_terms)

### Clear Example: Database Search

Let's use a database account search scenario as analogy:

- **DB_TERMS** = Field names that actually exist in the database:
  ```
  1. account_no (account number)
  2. account_id (account ID)
  3. transaction (transaction history)
  ```

- **CUSTOM_MAPPINGS** = Rules to convert STT output to DB field names:
  ```
  "ì–´ì¹´ìš´íŠ¸ë„˜ë²„" â†’ "account_no"
  "ì–´ì¹´ìš´íŠ¸ì•„ì´ë””" â†’ "account_id"
  "ì„œë²„ì´ë¦„" â†’ "server_name"
  ```

In this case:
- "ì–´ì¹´ìš´íŠ¸ë„˜ë²„" converts to "account_no" (success: field exists in DB)
- "ì–´ì¹´ìš´íŠ¸ì•„ì´ë””" converts to "account_id" (success: field exists in DB)
- "ì„œë²„ì´ë¦„" attempts to convert to "server_name" but fails (fail: "server_name" field doesn't exist in DB)

## API Documentation

### `PronunciationMapper` Class

#### `__init__(db_terms, threshold=None, custom_mappings=None)`
- **Parameters**:
  - `db_terms`: Database term list (list) - Lexical Search targets
  - `threshold`: Similarity threshold (default 0.6)
  - `custom_mappings`: User-defined mapping dictionary (dict) - Query Rewriting rules

#### `find_closest_term(query_term, threshold=None)`
- **Parameters**:
  - `query_term`: Input word to map (Query Rewriting target)
  - `threshold`: Similarity threshold (uses initialization value if not specified)
- **Returns**:
  - `(mapped_term, similarity_score)`: Tuple of mapped term and similarity score

#### `map_sentence(sentence)`
- **Parameters**:
  - `sentence`: Input sentence to map (Query Rewriting target)
- **Returns**:
  - Mapped sentence (string)

#### `add_custom_mapping(source_term, target_term)`
- **Parameters**:
  - `source_term`: Source word
  - `target_term`: Target word (DB term)
- **Returns**:
  - None

## Examples

See the [tests/](tests/) directory for sample examples.

## Contributing

1. Fork this repository.
2. Create a feature branch (`git checkout -b feature/amazing-feature`).
3. Commit your changes (`git commit -m 'Add some amazing feature'`).
4. Push the branch (`git push origin feature/amazing-feature`).
5. Open a Pull Request.

## License

This project is distributed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Required Packages

- Python 3.6+
- NumPy
- jamo (Korean jamo decomposition library)

---

This project was developed to facilitate seamless integration between ASR systems and databases.
**Through Query Rewriting**, it improves the accuracy of speech recognition results and serves as an essential preprocessing tool in **Lexical Search environments where proper nouns, technical terms, and database field names must be matched exactly**.

## Contact

For questions or feedback, please contact us through [GitHub Issues](https://github.com/hyeonsangjeon/pronunciation-mapper/issues).
